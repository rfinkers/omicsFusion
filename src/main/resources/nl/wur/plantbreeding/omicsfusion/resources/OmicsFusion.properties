# -- standard text --
omicsFusion.lastUpdate=Last update: RF/100111

# -- homepage --
omicsFusion.home.text=<p>omicsFusion is a tool in which different regression methods can be used to analyze typical ~omics situations with large numbers of variables and smaller numbers of samples. omicsFusion apply univariate regression and different regularized multivariate regression methods like ridge regression (RR), LASSO, elastic net (EN), principal component regression (PCR), partial least squares regression (PLS), sparse partial least squares regression (SPLS) and random forest regression (RF) to analyze datasets. The generated output contains the mean square error of prediction, goodness of fit, variable selection and the ranking of the variables.</p>

# -- about page --
omicsFusion.about.text=<p>omicsFusion is a tool in which different regression methods can be used to analyze typical ~omics situations with large numbers of variables and smaller numbers of samples. omicsFusion implements the methods as described by Acharajee et al (2010).</p><p>The omicsFusion web application is written and maintained by Richard Finkers. Please contact <a href="mailto:richard.finkers@gmail.com">me</a> for more information.</a>

# -- standard errors --
errors.header=<ul>
errors.prefix=<li>
errors.suffix=</li>
errors.footer=</ul>
# -- validator --
errors.invalid={0} is invalid.
errors.maxlength={0} can not be greater than {1} characters.
errors.minlength={0} can not be less than {1} characters.
errors.range={0} is not in the range {1} through {2}.
errors.required={0} is required.
errors.byte={0} must be an byte.
errors.date={0} is not a date.
errors.double={0} must be an double.
errors.float={0} must be an float.
errors.integer={0} must be an integer.
errors.long={0} must be an long.
errors.short={0} must be an short.
errors.creditcard={0} is not a valid credit card number.
errors.email={0} is an invalid e-mail address.
# -- other --
errors.cancel=Operation cancelled.
errors.detail={0}
errors.general=The process did not complete. Details should follow.
errors.token=Request could not be completed. Operation is not in sequence.

# -- welcome --
menu.home=Home
menu.submit=Submit analysis
menu.retrieve=Previous analysis
menu.methods=Analysis methods
menu.about=About
menu.rf=Random Forest
menu.lasso=Lasso
menu.pls=PLS
menu.pcr=PCR
menu.univariate=univariate
menu.en=Elastic net
menu.spls=SPLS
menu.svm=Support vector machine
omicsFusion.heading=omicsFusion

#Generic validation methods
requiredstring = ${getText(fieldName)} is required.
requiredemail = ${getText(fieldName)} is not in correct format.
requiredlength = ${getText(fieldName)} has less than the required number of characters.
requiredinteger = ${getText(fieldName)} should be numeric.

#Generic buton labels
button.reset=Reset selection
please.select=--Please select--

#Methods
meth.introduction=<p2>omicsFusion uses two different type of methodes, namenly regression methods and machine learning methods.
reg.meth=<h2>Regression Methods</h2><p>Regression methods are essentially curve-fitting approaches. When there is one response variable and one predictor variable, simple linear regression consists of finding the best straight line relating the response to the predictor variable. In case of multiple predictors, a hyperplane is fitted. The usual criterion, the least squares criterion, minimizes the sum of squared distances between the observed responses and the fitted responses from the regression model (Montgomery et al., 1991). We can represent the least squares criterion as:  TODO: image Where; y= response vector; \u03b2=regression coefficients; x=predictor variables</p>
reg.meth.univariate=<h3>Univariate regression</h3><p>Univariate regression was used as a reference. We compare the variable selection and ranking of variables in the multivariate regression methods to the results from the univariate regressions of flesh colour to each of the individual LC-MS peaks. Univariate regression with a FDR (False discovery rate) adjustment was done according to the procedure of Benjamini and Hochberg (1995).</p>
reg.meth.penalized=<h3>Penalization methods</h3><p>Shrinkage methods, also called penalization methods, impose a penalty on the size of the regression coefficients. The penalty term is also called a \u2018regularization parameter\u2019. We have grouped the methods according to the type of penalty applied to the regression coefficients. The mean square error (MSE) of a regression model can be decomposed into two components: the square of the bias (difference between the estimate and the expectation of a parameter) and the variance. In situations with high collinearity  (p>>n), regression models usually have a very large variance and the MSE will mainly be determined by this large variance. Therefore, in such situations it can be advantageous (lower MSE) to accept some bias if it is allows us to decrease the variance by considerable amount (Hastie et al., 2001). Penalization methods impose a bias by applying a penalty to the regression coefficients.</p>
reg.meth.penalized.cont=<h4>Continuous penalization methods</h4><p>In this category of regression methods, shrinkage factors can take any value between zero and infinity. LASSO, RR and EN belong to this category. The value of the shrinkage parameter decides the amount of penalty applied to the regression coefficients. We use tenfold cross validation (Hendriks et al., 2007) to choose the optimum penalty value.</p>
reg.meth.penalized.cont.rr=<h5>Ridge regression (RR)</h5><p>Ridge regression (Hoerl and Kennard, 1970) shrinks the regression coefficients by imposing a penalty on the sum of squares (L2 norm) of the regression coefficients. TODO: image  The left part of the term shown above is the usual least squares criterion. In the right part, \u03bb2 is a shrinkage factor applied to the sum of the squared values of the regression coefficients. The larger the value of \u03bb2 , the heavier the penalty on the regression coefficients and the more they are shrunk towards zero. In ridge regression all the regressor variables in the model since regression coefficients do not become exactly zero (that would be equivalent to variables dropping out of the regression model). Ridge gives equal weight to absolutely correlated variables in the data set (Hastie et al., 2001).</p>
reg.meth.penalized.cont.lasso=<h5>LASSO</h5><p>The LASSO (Least Absolute Sum of Squares Operator, Tibshirani,  1996) is another regularization method but here the penalty is applied to the sum of the absolute values of the regression coefficients, the L1 norm. Mathematically, we can write this in the following way: TODO Image. Again, the left part of the term is the normal least squares criterion. The right part now is the penalized sum of the absolute values of the regression coefficients. Similar to ridge regression, the shrinkage parameter (\u03bb1) has to be decided on and we use tenfold cross validation for this. Penalizing the absolute values of the regression coefficients has the effect that a number of the estimated coefficients will become exactly zero which means that some regressors drop out of the regression model so that a LASSO fitted model can consist of fewer variables than the total number of available regressor variables. In other words, LASSO can implicitly perform variable selection. The number of selected variables is upper limited by the numbers of samples (n). In case of absolutely correlated variables LASSO just selects one and ignores the rest in the group (Hastie et al., 2001).</p>
reg.meth.penalized.cont.en=<h5>Elastic net (EN) </h5><p>Elastic net (Zou and Hastie, 2005) is a combination of LASSO and ridge regression. It uses both a ridge penalty (penalty on the sum of the squares of the regression coefficients) and a LASSO penalty (on the sum of the absolute values of the regression coefficients): TODO: image In elastic net, we optimize both penalty parameters simultaneously using tenfold cross validation. Variable selection is encouraged by the LASSO penalty () and groups of correlated variables get similar regression coefficients. Groups of correlated variables are either in or out of the model (Zou and Hastie, 2005). In contrast with LASSO, the number of selected variables is not limited by the number of individuals.</p>
reg.meth.penalized.disc=<h4>Discrete penalization methods</h4><p>Partial least squares (PLS)  and Principal component regression (PCR) are based on latent variables or components which are linear combination of the original variables. For both methods it is essential to select the optimum numbers of latent components for prediction of the response variable. We used tenfold cross validation to choose the optimum number of latent components based on the smallest MSEP value. The number of latent components can only take discrete values, hence these methods are discrete penalization methods.</p>
reg.meth.penalized.disc.pls=<h5>Partial least squares (PLS) </h5><p>PLS (Wold, 1975; Geladi and Kowlaski, 1986; Hoskuldson, 1988) is a method to relate a vector of response or a matrix of responses to a matrix of regressor variables. Here, we are considering only a single response. PLS is a dimension reduction method like PCA, but it uses a different criterion: maximization of the covariance between the latent variables and the response. As a consequence, usually fewer components are required for prediction as compared to PCR. The optimum number of latent components is chosen by tenfold cross validation. Since the optimum number of latent components is a discrete number, this method is also a discrete penalization method. Like in PCR, latent variables in PLS are also uncorrelated. Hybrid penalization method: In this section, we consider a method in which two different types of penalties (continuous and discrete) are applied simultaneously.  </p>
reg.meth.penalized.disc.pcr=<h5>Principal component regression (PCR)</h5><p>Principal component regression is a combination of principal component analysis (PCA) and multiple linear regression. First, PCA is done on all original regressors and each component (latent variable) is represented by a linear combination of the original variables. The number of latent variables is chosen by tenfold cross validation and the response is regressed on the selected latent variables. These latent variables in PCA are uncorrelated. In PCR the principal components are found by maximization of the variance in the predictors; the covariance of the predictors with the response variable is not taken into account. </p>
reg.meth.penalized.cont.spls=<h5>Sparse partial least square (SPLS) </h5><p>SPLS (Chun and Keles, 2009) is a combination of two different penalties. The continuous penalty is a LASSO penalty and discrete penalization is achieved by PLS. Variable selection is achieved by LASSO and dimension reduction by PLS. The respective hyperparameters i.e. the number of PLS components and the size of the LASSO penalty are optimized simultaneously by tenfold cross validation. As in normal PLS, each of the latent components is a linear combination of the original variables. </p>
mach.meth=<h2>Machine learning methods</h2><p>The goal of machine learning is to build a computer system that can adapt and learn from experience (Dietterich, 1999). Machine learning methods can handle data which are not normally distributed whereas the methods mentioned above assume normality. Machine learning methods can also handle nonlinear relationships between response and predictor variables.</p>
mach.meth.svm=<h5>support vector machine (SVM)</h5><p>The support vector machine (SVM) (Vapnik, 1995) was originally developed in a classification (Demiriz et al., 2001) context  and maximizes predictive accuracy while avoiding overfitting (Hastie et al., 2001; Cristianini and Shawe-Taylor, 2000) to the data. However, the methodology can also be used in a regression model (Cristianini and Shawe-Taylor, 2000). Mathematically, given the input data {(x1, y1), \u2026., (xn,yn)}, we want to find a function which will fit the following equation: TODO: img Where w is a weight vector and b is a constant The goal of support vector regression (SVR) is to find a function f(x) that has at most \u03b5 deviation (Cristianini and Shawe-Taylor, 2000) from the actually obtained targets (response) for all the predictors, and at the same time minimizes the distance between predicted and target values. SVR does not encourage grouping or variable selection. The penalty error and slack variables (Cristianini and Shawe-Taylor, 2000) are optimized by tenfold cross validation.</p>
mach.meth.rf=<h5>Random Forest (RF)</h5><p>A random forest (Breiman, 2001) is a collection of unpruned decision trees (Hastie et al., 2001), usually developed for a classification purpose but this method can be applied in a regression context as well (Segal, 2004). A random forest model is typically made up of hundreds of decision trees. Each decision tree is built from bootstrap samples of the data set. That is, some samples will be included more than once in the bootstrap sample, and others will not appear at all. Generally, about two thirds of the samples will be included in the subset of the training dataset, and one third will be left out (called the out-of-bag samples or OOB samples). In regression the prediction error is the average value over OOB predictions. Variable importance (Breiman, 2001) can be quantified in random forest regression. Variables used which decrease the prediction error obtain a higher variable importance. The number of variables randomly sampled as candidates at each split in the decision trees optimized. RF consists of 500 trees.</p>
